# -*- coding: utf-8 -*-
"""CodeAlpha_CreditScoringModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11MofgPZxmLDlASMYatryjYf3HowDO4kh
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

"""# **Understand The Training Data**

> From the train and test dataset, analysing the datas to evaluate the data.


"""

train_df.head(5)

train_df.tail(5)

train_df.shape

train_df.info()

train_df.columns

"""# **Describe Dataset**"""

train_df.describe()

"""**Here it summerise the statistics like count, mean, std, min, max, 25%, 50%, 75% for numeric columns.**"""

train_df.describe().transpose()

"""**Now, going to check the unique values and frequency test**"""

train_df.nunique()

"""Credit Score && Credit Mix column has respectively 3 and 4 unique values."""

train_df['Credit_Score'].value_counts()

train_df['Credit_Mix'].value_counts(normalize=True)

"""# **Cleaning DataSet**

> There are many null values , wrong datas. So, have to clear that datas to analays the datas Properly.


"""

train_df.columns[train_df.isnull().any()]

train_df.isnull().sum()

train_df.duplicated().sum()

x = train_df["Monthly_Inhand_Salary"].mean()

train_df.fillna({"Monthly_Inhand_Salary": x}, inplace=True)
# train_df['Monthly_Inhand_Salary'].isnull().sum()

x = train_df["Num_Credit_Inquiries"].mean()

train_df.fillna({"Num_Credit_Inquiries": x}, inplace=True)

"""Now, Based On ***`Credit_Score`*** Evaluating the all datas"""

# List of feature columns to plot
titles = [
    'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card',
    'Interest_Rate', 'Delay_from_due_date', 'Num_Credit_Inquiries',
    'Credit_Utilization_Ratio', 'Total_EMI_per_month'
]

# Setup a 2x4 layout instead of 3x3 (because we have 8 plots)
fig, ax = plt.subplots(2, 4, figsize=(24, 12))
ax = ax.flatten()

# Loop through features and create boxplots
for i, feature in enumerate(titles):
    sns.boxplot(
        x='Credit_Score',
        y=feature,
        data=train_df,
        ax=ax[i],
        showfliers=False,  # Hide outliers for cleaner visuals
    )
    ax[i].set_title(feature, fontsize=14, fontweight='bold')
    ax[i].set_xlabel('')
    ax[i].set_ylabel('')
    ax[i].grid(True, linestyle='--', alpha=0.5)

# Main title
fig.suptitle('Relationship between Credit Score and Different Features',
             fontsize=18, fontweight='bold')

# Adjust spacing between subplots
fig.subplots_adjust(hspace=0.3, wspace=0.3)

# Display the plots
plt.tight_layout(rect=[0, 0, 1, 0.95])  # Leave space for suptitle
plt.show()

"""# **Target Variable Analysis**

> Understanding the distribution and check for skewness


"""

train_df['Credit_Score'].hist()

train_df['Credit_Mix'].hist()

"""# ***Train Test Split***"""

X = train_df[[
    'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card',
    'Interest_Rate', 'Delay_from_due_date', 'Num_Credit_Inquiries',
    'Credit_Utilization_Ratio', 'Total_EMI_per_month' ]].values
Y = train_df[['Credit_Score']].values

x = test_df[[
    'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card',
    'Interest_Rate', 'Delay_from_due_date', 'Num_Credit_Inquiries',
    'Credit_Utilization_Ratio', 'Total_EMI_per_month' ]].values

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_encoded = le.fit_transform(Y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42, stratify=Y )

"""# **RandomForestClassification**

***Model Training***
"""

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=300, class_weight='balanced')

model.fit(X_train, y_train)

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

train_predictions = model.predict(X_test)

print('Accuracy Score', accuracy_score(y_test, train_predictions))

print('Classification Report', classification_report(y_test, train_predictions, target_names=le.classes_))

print('Confusion Matrix', confusion_matrix(y_test, train_predictions))
print('\n')

"""# ***Model Testing with Test Dataset***"""

test_predictions = model.predict(x)

# print('Accuracy Score', accuracy_score(y_test, test_predictions))
print(pd.Series(test_predictions).value_counts())

"""# **Submission File Create Where According to Customer ID the Credit Scoring Values will be stored into .CSV file. **"""

if 'Customer_ID' in test_df.columns:
  submission_df = pd.DataFrame({
      'Customer_ID': test_df['Customer_ID'],
      'Credit_Score': test_predictions
  })
  # Save the submission DataFrame to a CSV file
  submission_df.to_csv('submission.csv', index=False)

  print("Submission file 'submission.csv' created successfully.")
  print(submission_df.head(100))
else:
  submission_df = pd.DataFrame({
      'Credit_Score': test_predictions
  })

"""This code first ensures the test data is preprocessed correctly. Then, it creates a standard submission CSV file with the IDs and corresponding predictions. Finally, it shows how you can add the predictions as a new column directly to your test_df for internal use or further analysis, which aligns with the "Deployment/Real-world Use" scenario you mentioned. Remember to replace 'Customer_ID' with the actual name of the ID column in your test.csv file if it's different."""

# Create a copy to avoid SettingWithCopyWarning if you modify the original test_df later
test_df_with_predictions = test_df.copy()

# Add the predictions as a new column
test_df_with_predictions['Predicted_Credit_Score'] = test_predictions

print("\nTest DataFrame with predictions added:")
print(test_df_with_predictions.head())

"""# **ROC Curve && ROC_AUC_Score**

the standard sklearn roc_curve and roc_auc_score functions are designed for binary classification (two classes, e.g., 0 and 1) or multiclass problems framed as one-vs-rest. This problem is multiclass classification with three classes ('Good', 'Standard', 'Poor')

For multiclass problems, you can generate ROC curves and calculate AUC scores using strategies like:

1.   One-vs-Rest (OvR): Calculate the ROC curve and AUC for each class against all other classes combined.
2.   One-vs-One (OvO): Calculate the ROC curve and AUC for each pair of classes.


The most common approach for visualization and reporting in multiclass is One-vs-Rest. We'll use that method.
"""

from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.preprocessing import LabelBinarizer

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.preprocessing import LabelBinarizer # Useful for OvR

# --- Get Predicted Probabilities for the Validation Set (X_test) ---
# The model should be trained. We will use the probabilities on the X_test set
# as this is the set where you have the true labels (y_test) for evaluation.
y_pred_proba = model.predict_proba(X_test)

# --- Encode True Labels for OvR ---
# Use the LabelEncoder you already fitted on Y (the training targets)
# to transform y_test into numerical labels.
y_test_encoded = le.transform(y_test.ravel()) # .ravel() to flatten y_test

# Since it's a multiclass problem, we need to binarize the true labels
# into a one-vs-rest format for calculating OvR AUC.
lb = LabelBinarizer()
y_test_binarized = lb.fit_transform(y_test_encoded)

# Get the class names from the LabelEncoder
class_names = le.classes_
n_classes = len(class_names)

# --- Calculate ROC curve and AUC for each class (One-vs-Rest) ---
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(n_classes):
    # roc_curve expects true binary labels and predicted probabilities for the positive class
    # y_test_binarized[:, i] gives the binary true labels for class i (1 if it's class i, 0 otherwise)
    # y_pred_proba[:, i] gives the predicted probabilities of class i
    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_proba[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# --- Plot the ROC curves for each class ---
plt.figure(figsize=(10, 8))

colors = ['blue', 'red', 'green'] # Define colors for each class

for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f'ROC curve of class {class_names[i]} (area = {roc_auc[i]:0.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance') # Plot the random chance line (AUC = 0.5)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (One-vs-Rest)')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

# --- Calculate Micro and Macro Average AUC (Optional but Recommended) ---
# Micro-average AUC
# Flattens the binarized true labels and predicted probabilities
micro_auc = roc_auc_score(y_test_binarized.ravel(), y_pred_proba.ravel(), multi_class='ovr')
print(f"\nMicro-average ROC AUC: {micro_auc:0.2f}")

# Macro-average AUC
# Calculates the unweighted mean of AUC for each class
macro_auc = roc_auc_score(y_test_binarized, y_pred_proba, multi_class='ovr', average='macro')
print(f"Macro-average ROC AUC: {macro_auc:0.2f}")